{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vrushali Shah\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.didPurchase = (df.didPurchase)*1\n",
    "df.doRecommend = (df.doRecommend)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['didPurchase'] = df['didPurchase'].fillna(1)\n",
    "df['doRecommend'] = df['doRecommend'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56802, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = df[['username','rating','doRecommend','didPurchase']]\n",
    "#training_set=training_set.iloc[0:10000,:]\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.didPurchase = (df.didPurchase)*1\n",
    "df.doRecommend = (df.doRecommend)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['didPurchase'] = df['didPurchase'].fillna(1)\n",
    "df['doRecommend'] = df['doRecommend'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14201, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = df[['username','rating','doRecommend','didPurchase']]\n",
    "#test_set=test_set.iloc[0:10000,:]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "#training_set['rating'] = sc.fit_transform(training_set[['rating']])\n",
    "#print(training_set['didPurchase'])\n",
    "#training_set['didPurchase'] = sc.fit_transform(training_set[['didPurchase']])\n",
    "#print(training_set['didPurchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "training_steps = 200\n",
    "batch_size = 300\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 3 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 1 # timesteps\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input], name=\"x\")\n",
    "Y = tf.placeholder(\"float\", [None, num_classes], name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]),name=\"W\")\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]),name=\"B\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.summary.histogram(\"weights\", weights)\n",
    "#tf.summary.histogram(\"biases\", biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "   \n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    #lstm_cell=rnn.MultiRNNCell([rnn.BasicLSTMCell(num_hidden),rnn.BasicLSTMCell(num_hidden)])\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = RNN(X, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "def start_training(flag=True):\n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./log_summaries\")\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "        display_step = training_steps/10\n",
    "        \n",
    "        for step in range(1, training_steps+1):\n",
    "            #X_train, X_test, y_train, y_test = train_test_split(training_set[['rating','didPurchase']], training_set[['doRecommend']], test_size=0.2)\n",
    "            #batch_x, batch_y = X_train, enc.fit_transform(y_train).toarray()[0:batch_size,:]\n",
    "            batch=training_set.sample(batch_size)\n",
    "            batch_x, batch_y=batch[['username','rating','didPurchase']], enc.fit_transform(batch[['doRecommend']]).toarray()\n",
    "            # Reshape data to get 28 seq of 28 elements\n",
    "            batch_x = batch_x.values.reshape((batch_size, timesteps, num_input))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            if (step % display_step == 0 or step == 1) and flag:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc, s = sess.run([loss_op, accuracy, summ], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                writer.add_summary(s, step)\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "            elif training_steps<=20 and flag:\n",
    "                loss, acc, s = sess.run([loss_op, accuracy, summ], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                writer.add_summary(s, step)\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "        if flag:\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for 128 mnist test images\n",
    "        test_len = 128\n",
    "        test_data =test_set[['username','rating','didPurchase']].values.reshape((-1, timesteps, num_input))\n",
    "        test_label = enc.fit_transform(test_set[['doRecommend']]).toarray()#[:test_data.shape[0],:]\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "        writer.add_graph(sess.graph)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 100\n",
    "# activation function = Softmax\n",
    "# cost function = cross entropy\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_tanh_RMS:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.tanh(logits)\n",
    "tf.summary.histogram(\"tanh\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_tanh_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 3.3334, Training Accuracy= 0.053\n",
      "Step 20, Minibatch Loss= 3.3035, Training Accuracy= 0.077\n",
      "Step 40, Minibatch Loss= 3.3000, Training Accuracy= 0.070\n",
      "Step 60, Minibatch Loss= 3.2752, Training Accuracy= 0.083\n",
      "Step 80, Minibatch Loss= 3.2869, Training Accuracy= 0.053\n",
      "Step 100, Minibatch Loss= 3.2872, Training Accuracy= 0.047\n",
      "Step 120, Minibatch Loss= 3.2421, Training Accuracy= 0.080\n",
      "Step 140, Minibatch Loss= 3.2371, Training Accuracy= 0.067\n",
      "Step 160, Minibatch Loss= 3.2340, Training Accuracy= 0.063\n",
      "Step 180, Minibatch Loss= 3.2086, Training Accuracy= 0.077\n",
      "Step 200, Minibatch Loss= 3.2127, Training Accuracy= 0.060\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.068023376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 100\n",
    "# activation function = sigmoid\n",
    "# cost function = sigmoid cross entropy\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_sigmoid_RMS:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmoid\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_sigmoid_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.1779, Training Accuracy= 0.097\n",
      "Step 20, Minibatch Loss= 1.1952, Training Accuracy= 0.063\n",
      "Step 40, Minibatch Loss= 1.1532, Training Accuracy= 0.093\n",
      "Step 60, Minibatch Loss= 1.1475, Training Accuracy= 0.067\n",
      "Step 80, Minibatch Loss= 1.1106, Training Accuracy= 0.090\n",
      "Step 100, Minibatch Loss= 1.0989, Training Accuracy= 0.073\n",
      "Step 120, Minibatch Loss= 1.0652, Training Accuracy= 0.093\n",
      "Step 140, Minibatch Loss= 1.0511, Training Accuracy= 0.077\n",
      "Step 160, Minibatch Loss= 1.0270, Training Accuracy= 0.080\n",
      "Step 180, Minibatch Loss= 1.0101, Training Accuracy= 0.070\n",
      "Step 200, Minibatch Loss= 0.9808, Training Accuracy= 0.087\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.068023376\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 100\n",
    "# activation function = relu\n",
    "# cost function = sigmoid cross entropy\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_relu_RMS:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.relu(logits)\n",
    "tf.summary.histogram(\"relu\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_relu_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.2986, Training Accuracy= 0.057\n",
      "Step 20, Minibatch Loss= 1.2578, Training Accuracy= 0.057\n",
      "Step 40, Minibatch Loss= 1.1759, Training Accuracy= 0.093\n",
      "Step 60, Minibatch Loss= 1.1635, Training Accuracy= 0.047\n",
      "Step 80, Minibatch Loss= 1.0762, Training Accuracy= 0.090\n",
      "Step 100, Minibatch Loss= 1.0492, Training Accuracy= 0.063\n",
      "Step 120, Minibatch Loss= 0.9934, Training Accuracy= 0.077\n",
      "Step 140, Minibatch Loss= 0.9509, Training Accuracy= 0.070\n",
      "Step 160, Minibatch Loss= 0.9127, Training Accuracy= 0.053\n",
      "Step 180, Minibatch Loss= 0.8670, Training Accuracy= 0.057\n",
      "Step 200, Minibatch Loss= 0.8238, Training Accuracy= 0.060\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.068023376\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions used relu, tanh, sigmoid, softmax\n",
    "\n",
    "The accuracy achieved with test data with all four activation function was quite good accuracy of ~ 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 100\n",
    "# activation function = sigmoid\n",
    "# cost function = mean_squared_error\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_MSE_RMS:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"MSE\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.losses.mean_squared_error(logits,prediction)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_MSE_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 8.7318, Training Accuracy= 0.067\n",
      "Step 20, Minibatch Loss= 8.4524, Training Accuracy= 0.043\n",
      "Step 40, Minibatch Loss= 8.2108, Training Accuracy= 0.060\n",
      "Step 60, Minibatch Loss= 7.9773, Training Accuracy= 0.063\n",
      "Step 80, Minibatch Loss= 7.7481, Training Accuracy= 0.070\n",
      "Step 100, Minibatch Loss= 7.5229, Training Accuracy= 0.047\n",
      "Step 120, Minibatch Loss= 7.3012, Training Accuracy= 0.073\n",
      "Step 140, Minibatch Loss= 7.0834, Training Accuracy= 0.073\n",
      "Step 160, Minibatch Loss= 6.8693, Training Accuracy= 0.053\n",
      "Step 180, Minibatch Loss= 6.6590, Training Accuracy= 0.040\n",
      "Step 200, Minibatch Loss= 6.4524, Training Accuracy= 0.043\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.06710795\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 100\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_sigmoid_RMS_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmoid\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.losses.hinge_loss(logits,prediction)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_sigmoid_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.8509, Training Accuracy= 0.903\n",
      "Step 20, Minibatch Loss= 0.8218, Training Accuracy= 0.920\n",
      "Step 40, Minibatch Loss= 0.7846, Training Accuracy= 0.937\n",
      "Step 60, Minibatch Loss= 0.7695, Training Accuracy= 0.940\n",
      "Step 80, Minibatch Loss= 0.7644, Training Accuracy= 0.947\n",
      "Step 100, Minibatch Loss= 0.7592, Training Accuracy= 0.940\n",
      "Step 120, Minibatch Loss= 0.7540, Training Accuracy= 0.930\n",
      "Step 140, Minibatch Loss= 0.7488, Training Accuracy= 0.937\n",
      "Step 160, Minibatch Loss= 0.7469, Training Accuracy= 0.960\n",
      "Step 180, Minibatch Loss= 0.7386, Training Accuracy= 0.950\n",
      "Step 200, Minibatch Loss= 0.7335, Training Accuracy= 0.967\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9329625\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested RNN with MSE and hinge loss both gave ~ 93% accuracy. Thus, there wasn't any effect with change in loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = varying\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_MSE_RMS_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmoid\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.losses.hinge_loss(logits,prediction)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_MSE_RMS\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1000\n",
      "Testing Accuracy: 0.9329625\n",
      "------------------------------\n",
      "Epochs: 2000\n",
      "Testing Accuracy: 0.93218786\n",
      "------------------------------\n",
      "Epochs: 3000\n",
      "Testing Accuracy: 0.06781212\n",
      "------------------------------\n",
      "Epochs: 4000\n",
      "Testing Accuracy: 0.93275124\n",
      "------------------------------\n",
      "Epochs: 5000\n",
      "Testing Accuracy: 0.06703753\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,5500,1000):\n",
    "    print(\"Epochs: %i\" % i)\n",
    "    training_steps=i\n",
    "    start_training(False)\n",
    "    print(30 *\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch size = 300\n",
    "# num of hidden features = 1000\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_sigmoid_Adagrad:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmiod\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_sigmoid_Adagrad\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.9417, Training Accuracy= 0.917\n",
      "Step 100, Minibatch Loss= 0.8710, Training Accuracy= 0.943\n",
      "Step 200, Minibatch Loss= 0.8722, Training Accuracy= 0.940\n",
      "Step 300, Minibatch Loss= 0.9103, Training Accuracy= 0.920\n",
      "Step 400, Minibatch Loss= 0.9000, Training Accuracy= 0.923\n",
      "Step 500, Minibatch Loss= 0.8442, Training Accuracy= 0.947\n",
      "Step 600, Minibatch Loss= 0.8493, Training Accuracy= 0.943\n",
      "Step 700, Minibatch Loss= 0.9124, Training Accuracy= 0.913\n",
      "Step 800, Minibatch Loss= 0.8363, Training Accuracy= 0.947\n",
      "Step 900, Minibatch Loss= 0.9071, Training Accuracy= 0.913\n",
      "Step 1000, Minibatch Loss= 0.8607, Training Accuracy= 0.933\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9329625\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch size = 300\n",
    "# num of hidden features = 1000\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_sigmoid_AdaDelta:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmoid\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_sigmoid_AdaDelta\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.4375, Training Accuracy= 0.933\n",
      "Step 100, Minibatch Loss= 1.4864, Training Accuracy= 0.917\n",
      "Step 200, Minibatch Loss= 1.4570, Training Accuracy= 0.927\n",
      "Step 300, Minibatch Loss= 1.5547, Training Accuracy= 0.893\n",
      "Step 400, Minibatch Loss= 1.4471, Training Accuracy= 0.930\n",
      "Step 500, Minibatch Loss= 1.3982, Training Accuracy= 0.947\n",
      "Step 600, Minibatch Loss= 1.3786, Training Accuracy= 0.953\n",
      "Step 700, Minibatch Loss= 1.3785, Training Accuracy= 0.953\n",
      "Step 800, Minibatch Loss= 1.4077, Training Accuracy= 0.943\n",
      "Step 900, Minibatch Loss= 1.4468, Training Accuracy= 0.930\n",
      "Step 1000, Minibatch Loss= 1.4142, Training Accuracy= 0.940\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9329625\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 1000\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_sigmoid_GradientDescent:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.sigmoid(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.losses.hinge_loss(logits,prediction)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "prediction = tf.nn.sigmoid(logits)\n",
    "tf.summary.histogram(\"sigmoid\", prediction)\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss_op = tf.losses.hinge_loss(logits,prediction)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"accuracy_sigmoid_GradientDescent\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.9043, Training Accuracy= 0.053\n",
      "Step 100, Minibatch Loss= 0.9043, Training Accuracy= 0.050\n",
      "Step 200, Minibatch Loss= 0.9042, Training Accuracy= 0.050\n",
      "Step 300, Minibatch Loss= 0.9042, Training Accuracy= 0.060\n",
      "Step 400, Minibatch Loss= 0.9042, Training Accuracy= 0.060\n",
      "Step 500, Minibatch Loss= 0.9041, Training Accuracy= 0.067\n",
      "Step 600, Minibatch Loss= 0.9041, Training Accuracy= 0.083\n",
      "Step 700, Minibatch Loss= 0.9055, Training Accuracy= 0.103\n",
      "Step 800, Minibatch Loss= 0.9040, Training Accuracy= 0.107\n",
      "Step 900, Minibatch Loss= 0.9040, Training Accuracy= 0.053\n",
      "Step 1000, Minibatch Loss= 0.9039, Training Accuracy= 0.080\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.068023376\n"
     ]
    }
   ],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy changed for GradientDescent and AdaDelta but was same for Adagrad optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch size = 300\n",
    "# num of hidden features = 1000\n",
    "# activation function = sigmoid\n",
    "# cost function = hinge loss\n",
    "# gradient estimator = GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of features: 50\n",
      "Testing Accuracy: 0.9329625\n",
      "------------------------------\n",
      "Num of features: 100\n",
      "Testing Accuracy: 0.06703753\n",
      "------------------------------\n",
      "Num of features: 150\n",
      "Testing Accuracy: 0.06703753\n",
      "------------------------------\n",
      "Num of features: 200\n",
      "Testing Accuracy: 0.068023376\n",
      "------------------------------\n",
      "Num of features: 250\n",
      "Testing Accuracy: 0.068023376\n",
      "------------------------------\n",
      "Num of features: 300\n",
      "Testing Accuracy: 0.9322583\n",
      "------------------------------\n",
      "Num of features: 350\n",
      "Testing Accuracy: 0.06703753\n",
      "------------------------------\n",
      "Num of features: 400\n",
      "Testing Accuracy: 0.9329625\n",
      "------------------------------\n",
      "Num of features: 450\n",
      "Testing Accuracy: 0.06703753\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(50,500,50):\n",
    "    print(\"Num of features: %i\" % i)\n",
    "    num_hidden=i\n",
    "    start_training(False)\n",
    "    print(30 *\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The num of hidden features does affect the accuracy. Accuracy is low for any value below 200 but increases on 200 and stays the same for values 200 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.1865, Training Accuracy= 0.923\n",
      "Step 100, Minibatch Loss= 0.0770, Training Accuracy= 0.930\n",
      "Step 200, Minibatch Loss= 0.0000, Training Accuracy= 0.937\n",
      "Step 300, Minibatch Loss= 0.0025, Training Accuracy= 0.913\n",
      "Step 400, Minibatch Loss= 0.0000, Training Accuracy= 0.940\n",
      "Step 500, Minibatch Loss= 0.0025, Training Accuracy= 0.953\n",
      "Step 600, Minibatch Loss= 0.0000, Training Accuracy= 0.930\n",
      "Step 700, Minibatch Loss= 0.0025, Training Accuracy= 0.927\n",
      "Step 800, Minibatch Loss= 0.0000, Training Accuracy= 0.957\n",
      "Step 900, Minibatch Loss= 0.0053, Training Accuracy= 0.930\n",
      "Step 1000, Minibatch Loss= 0.0000, Training Accuracy= 0.957\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9329625\n"
     ]
    }
   ],
   "source": [
    "# xavier Initializer\n",
    "weights = {\n",
    "    'out':  tf.get_variable(\"W1\", shape=[num_hidden, num_classes],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "}\n",
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.8188, Training Accuracy= 0.083\n",
      "Step 100, Minibatch Loss= 0.8129, Training Accuracy= 0.047\n",
      "Step 200, Minibatch Loss= 0.8099, Training Accuracy= 0.057\n",
      "Step 300, Minibatch Loss= 0.8069, Training Accuracy= 0.083\n",
      "Step 400, Minibatch Loss= 0.8068, Training Accuracy= 0.083\n",
      "Step 500, Minibatch Loss= 0.8008, Training Accuracy= 0.057\n",
      "Step 600, Minibatch Loss= 0.7978, Training Accuracy= 0.073\n",
      "Step 700, Minibatch Loss= 0.7978, Training Accuracy= 0.050\n",
      "Step 800, Minibatch Loss= 0.7946, Training Accuracy= 0.063\n",
      "Step 900, Minibatch Loss= 0.7886, Training Accuracy= 0.073\n",
      "Step 1000, Minibatch Loss= 0.7855, Training Accuracy= 0.087\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.06703753\n"
     ]
    }
   ],
   "source": [
    "# zeros initializer\n",
    "weights = {\n",
    "    'out':  tf.get_variable(\"W2\", shape=[num_hidden, num_classes],\n",
    "           initializer=tf.zeros_initializer())\n",
    "}\n",
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy stayed about the same for xavier initialization but changed drastically low for zero initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
